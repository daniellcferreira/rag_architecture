{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JojQwFLQpKK6",
        "outputId": "d4d69003-cde1-4bd2-bea0-ce4081d23216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/131.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m122.9/131.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Instala a biblioteca Groq, usada para acessar modelos de linguagem hospedados na Groq\n",
        "!pip install -q groq\n",
        "\n",
        "# Instala a biblioteca SentenceTransformers, usada para criar embeddings (representações vetoriais de textos)\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "# Instala a biblioteca Wikipedia-API, que permite buscar e manipular conteúdo da Wikipedia\n",
        "!pip install -q wikipedia-api\n",
        "\n",
        "# Instala/atualiza um conjunto de bibliotecas da LangChain e utilitários extras:\n",
        "# - langchain-nvidia-ai-endpoints: integração da LangChain com modelos da NVIDIA\n",
        "# - langchain-community: integrações mantidas pela comunidade (APIs, ferramentas, modelos)\n",
        "# - langchain: framework principal para criar aplicações com LLMs\n",
        "# - langgraph: extensão para estruturar fluxos conversacionais e agentes em forma de grafo\n",
        "# - tavily-python: integração com a API de busca Tavily (busca na web)\n",
        "# - beautifulsoup4: biblioteca para raspagem e análise de HTML\n",
        "# - lxml: parser rápido para XML/HTML, usado junto com o BeautifulSoup\n",
        "!pip install -qU langchain-nvidia-ai-endpoints langchain-community langchain langgraph tavily-python beautifulsoup4 lxml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bibliotecas padrão\n",
        "import json          # Para manipulação de dados em formato JSON\n",
        "import os            # Para interagir com variáveis de ambiente e sistema de arquivos\n",
        "import numpy as np   # Para cálculos numéricos e manipulação de arrays\n",
        "import operator      # Para operadores úteis em comparações e ordenações\n",
        "\n",
        "# Colab\n",
        "from google.colab import userdata  # Para acessar secrets/variáveis de ambiente salvas no Colab\n",
        "\n",
        "# Groq\n",
        "from groq import Groq  # Cliente para acessar modelos de linguagem hospedados na Groq\n",
        "\n",
        "# IPython\n",
        "from IPython.display import Image, display  # Exibir imagens diretamente no notebook\n",
        "\n",
        "# LangChain - utilitários principais\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Quebra textos em chunks para embeddings\n",
        "\n",
        "# LangChain Community - loaders e ferramentas adicionais\n",
        "from langchain_community.document_loaders import WebBaseLoader   # Carrega documentos a partir de páginas web\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults  # Ferramenta para buscar resultados na web via Tavily\n",
        "\n",
        "# LangChain Core\n",
        "from langchain_core.vectorstores import InMemoryVectorStore  # Vetoriza e armazena embeddings em memória\n",
        "from langchain_core.messages import HumanMessage, SystemMessage  # Estrutura mensagens para interações com LLMs\n",
        "\n",
        "# LangChain NVIDIA\n",
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings, ChatNVIDIA\n",
        "# - NVIDIAEmbeddings: gera embeddings usando modelos NVIDIA\n",
        "# - ChatNVIDIA: interage com modelos de chat NVIDIA\n",
        "\n",
        "# LangChain - estrutura de documentos\n",
        "from langchain.schema import Document  # Representa documentos que serão usados no pipeline\n",
        "\n",
        "# LangGraph\n",
        "from langgraph.graph import END, StateGraph\n",
        "# - END: marcador para finalização de um grafo\n",
        "# - StateGraph: estrutura de grafos de estados para agentes/conversas\n",
        "\n",
        "# Pydantic\n",
        "from pydantic import BaseModel, Field  # Para definição de modelos de dados com validação\n",
        "\n",
        "# Sentence Transformers\n",
        "from sentence_transformers import SentenceTransformer  # Modelo de embeddings baseado em transformadores\n",
        "\n",
        "# Tipagem avançada\n",
        "from typing_extensions import TypedDict, List, Annotated, Literal\n",
        "# Tipos auxiliares para melhorar clareza de estados e estruturas\n",
        "\n",
        "# Wikipedia API\n",
        "from wikipediaapi import Wikipedia  # API para buscar e manipular páginas da Wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvnR7NVbq16s",
        "outputId": "c20a7684-97b4-43ec-f84a-73cd7f3fdce8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ativa o tracing (rastreamento) da execução no LangSmith para monitorar os fluxos do LangChain\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"True\"\n",
        "\n",
        "# Define o endpoint da API do LangSmith (plataforma de monitoramento do LangChain)\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "\n",
        "# Configura a chave de API do LangSmith (armazenada nos secrets do Colab)\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "\n",
        "# Nome do projeto dentro do LangSmith para organizar os rastreamentos e execuções\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"pr-crushing-nexus-98\"\n",
        "\n",
        "# Configura a chave de API da Tavily (serviço de busca na web), também obtida dos secrets do Colab\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')\n",
        "\n",
        "# Configura a chave de API da NVIDIA (para usar modelos de embeddings e chat NVIDIA)\n",
        "os.environ[\"NVIDIA_API_KEY\"] = userdata.get('NVIDIA_API_KEY')"
      ],
      "metadata": {
        "id": "R0bXTFqBscU9"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}